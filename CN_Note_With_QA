Q1：KNN的思想是什么？它与深度学习方法的区别是什么？
  A1：
  <1>KNN是基于距离的分类算法，KNN的思想就是给定一个训练数据集，当要给测试数据进行分类时，就计算该数据与所有训练数据的距离，并按照得到的距离进行排序并对前K名求加权。加权第一名就是预测的分类结果。
  <2>距离可以用L1距离与L2距离（欧氏距离）或其他方法。KNN的性能主要取决于距离如何定义。
  <3>KNN算法形象、易于理解。K值取的较高时，可以使划分的决策边界较为平滑，缺点在于它会使测试模型的运算也变得缓慢。
  <4>要提升KNN的性能，应主要着力于调整超参数K和距离度量公式的选择。
  在KNN中，如果数据以向量表示，比如一个学生有学号、姓名、性别等几乎相互独立的维度，则采取L1距离较为合适；维度间相关度较强时，使用L2距离较为合适。
  
 Q2：对KNN关于超参数的一些理解
  A2：
  <1>超参数就是与训练数据集无关的、需要事先人为地为模型设置的参数。例如KNN中的K。
  <2>我认为KNN属于传统机器学习的原因，因为它不需要通过数据集训练神经网络参数，甚至它根本不是基于神经网络的算法。
  <3>在训练时，期待KNN在验证集上的表现性能达到最佳，找到这个超参数K，再应用到测试集上。而不是直接通过测试集寻找K，直接用测试集寻找超参数意味着丢失了训练过程。
  <4>如果一定说KNN有训练过程，可以认为KNN的验证集和测试集是两个相互独立的测试集，它缺少神经网络算法的训练过程。
  
 Q3：为什么图像分类很少使用KNN？
  A3：
  <1>距离的度量难以准确的刻画图像之间的相似度，比如一张图片在保留目标内容的前提下，对背景进行染色，它们之间的L1、L2距离将会变大。然而其实它们所表达的语义几乎一致，这会影响KNN分类的性能。
  <2>为了达到高精度，需要做到将样本空间全覆盖。KNN其实是用训练数据将样本空间分成Classes个块（Classes = 分类数），因此KNN所需要的数据量是指数级增长的。
  <3>在测试时，KNN的速度很慢。而深度学习模型在训练时很慢，一旦确定好网络参数，它的测试速度就是前向传播的速度了，与KNN相比效率更高。
  
 Q4：什么是线性分类器？
  A4：
  <1>无论网络有多少层神经元，如果我们忽略中间各层之间的映射细节，其实可以把这个网络视为一个从输入到输出的一个极其复杂的函数映射 f(x,W)
  <2>当映射函数十分简单时，比如只有一层且不使用激活函数，则它可以视为一个线性分类器。例如，一个X(32,32,3)的RGB图像可以通过线性分类器分为10类，这与PCA映射到新空间的方法思想上是类似的，将其通过变换矩阵变换到10个基撑起的向量空间即可。
  当然，深度学习模型的变换矩阵还是要通过训练数据基于反向传播训练出来的。
  <3>线性分类器的局限性在于它每次只能做一次二分类。假如数据集有10类，应用某个分类器只能得知它属于这个类亦或是它不属于这个类。因此n多分类任务至少要调用n-1次线性分类器。
